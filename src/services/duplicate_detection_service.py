#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DuplicateDetectionService - —É–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π.

–≠—Ç–æ—Ç —Å–µ—Ä–≤–∏—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:
1. –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä (—Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ 24 —á–∞—Å–∞)
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–æ—á–∏—Å—Ç–∫–∞, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
3. –ê–ª–≥–æ—Ä–∏—Ç–º –ú–∞–π–µ—Ä—Å–∞ + –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω (–±—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
4. RuBERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
5. DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π)

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 1 –Ω–æ–≤–æ—Å—Ç–∏ —Å 12 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏: ~0.1 —Å–µ–∫
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 1 –Ω–æ–≤–æ—Å—Ç–∏ —Å 50 –Ω–æ–≤–æ—Å—Ç—è–º–∏: ~0.5 —Å–µ–∫
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ 10 —Ä–∞–∑
"""

import asyncio
import logging
import re
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch

from src.config.settings import config
from src.services.database_singleton import get_database_service
from src.services.sqlite_cache_service import SQLiteCache

logger = logging.getLogger(__name__)


@dataclass
class DuplicateResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
    is_duplicate: bool
    existing_news_id: Optional[int] = None
    similarity_score: float = 0.0
    similarity_type: str = ""  # "myers", "cosine", "cluster"
    reason: str = ""
    cluster_id: Optional[int] = None
    sources_to_merge: List[int] = None


class DuplicateDetectionService:
    """
    –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–¥—Ö–æ–¥:
    1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    2. –ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∞–ª–≥–æ—Ä–∏—Ç–º –ú–∞–π–µ—Ä—Å–∞
    3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RuBERT
    4. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        self.config = config.duplicate_detection
        self.db = get_database_service()
        self.cache = SQLiteCache()
        
        # RuBERT –º–æ–¥–µ–ª—å (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self._tokenizer = None
        self._model = None
        self._model_loaded = False
        
        logger.info("üîç DuplicateDetectionService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {self.config.time_window_hours}—á, "
                   f"–ø–æ—Ä–æ–≥ –ú–∞–π–µ—Ä—Å–∞: {self.config.myers_threshold}, "
                   f"–ø–æ—Ä–æ–≥ –∫–æ—Å–∏–Ω—É—Å–∞: {self.config.cosine_threshold}")
    
    async def detect_duplicates(
        self, 
        title: str, 
        content: str, 
        filter_relevant: bool = True
    ) -> DuplicateResult:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–æ–≤–æ—Å—Ç–∏
            filter_relevant: –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            DuplicateResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è: '{title[:50]}...'")
            
            # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            processed_text = await self._preprocess_text(f"{title} {content}")
            
            if len(processed_text) < self.config.min_text_length:
                logger.info(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(processed_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return DuplicateResult(
                    is_duplicate=False,
                    reason="–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
                )
            
            # 2. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            candidate_news = await self._get_candidate_news(filter_relevant)
            
            if not candidate_news:
                logger.info("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
                return DuplicateResult(
                    is_duplicate=False,
                    reason="–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
                )
            
            logger.info(f"üìä –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å {len(candidate_news)} –Ω–æ–≤–æ—Å—Ç—è–º–∏")
            
            # 3. –ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∞–ª–≥–æ—Ä–∏—Ç–º –ú–∞–π–µ—Ä—Å–∞
            myers_result = await self._myers_comparison(processed_text, candidate_news)
            if myers_result.is_duplicate:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–µ—Ä–µ–∑ –ú–∞–π–µ—Ä—Å–∞: {myers_result.similarity_score:.3f}")
                return myers_result
            
            # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RuBERT
            cosine_result = await self._cosine_similarity_comparison(processed_text, candidate_news)
            if cosine_result.is_duplicate:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {cosine_result.similarity_score:.3f}")
                return cosine_result
            
            # 5. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏)
            cluster_result = await self._cluster_similar_news(processed_text, candidate_news)
            if cluster_result.is_duplicate:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é: {cluster_result.similarity_score:.3f}")
                return cluster_result
            
            logger.info("‚ÑπÔ∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return DuplicateResult(
                is_duplicate=False,
                reason="–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            return DuplicateResult(
                is_duplicate=False,
                reason=f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"
            )
    
    async def _preprocess_text(self, text: str) -> str:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            str: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # 1. –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
            text = re.sub(r'<[^>]+>', '', text)
            
            # 2. –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
            text = re.sub(r'[^\w\s]', ' ', text)
            
            # 3. –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            text = text.lower()
            
            # 4. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            text = re.sub(r'\s+', ' ', text).strip()
            
            logger.debug(f"üìù –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return text
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return text
    
    async def _get_candidate_news(self, filter_relevant: bool = True) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        
        Args:
            filter_relevant: –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        """
        try:
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
            time_threshold = datetime.utcnow() - timedelta(hours=self.config.time_window_hours)
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î
            with self.db.get_session() as session:
                from src.models.database import News
                
                query = session.query(News).filter(
                    News.created_at >= time_threshold,
                    News.status != 'deleted'
                )
                
                # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
                if filter_relevant:
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–æ—Ü–µ–Ω–∫–∞ >= 6)
                    query = query.filter(News.ai_relevance_score >= 6)
                    logger.debug("üîç –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (ai_relevance_score >= 6)")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                query = query.limit(self.config.max_news_to_compare)
                
                news_list = query.all()
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
                candidates = []
                for news in news_list:
                    candidates.append({
                        'id': news.id,
                        'title': news.title or '',
                        'content': news.content or '',
                        'ai_summary': news.ai_summary or '',
                        'created_at': news.created_at
                    })
                
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
                return candidates
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {e}")
            return []
    
    async def _myers_comparison(self, text: str, candidates: List[Dict]) -> DuplicateResult:
        """
        –ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∞–ª–≥–æ—Ä–∏—Ç–º –ú–∞–π–µ—Ä—Å–∞ + –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω.
        
        Args:
            text: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            candidates: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            
        Returns:
            DuplicateResult: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        try:
            for candidate in candidates:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                candidate_text = f"{candidate['title']} {candidate['content']}"
                candidate_processed = await self._preprocess_text(candidate_text)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
                distance = self._levenshtein_distance(text, candidate_processed)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
                max_length = max(len(text), len(candidate_processed))
                if max_length == 0:
                    continue
                
                similarity = 1.0 - (distance / max_length)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥
                if similarity > (1.0 - self.config.myers_threshold):
                    logger.info(f"üéØ –ú–∞–π–µ—Ä—Å: —Å—Ö–æ–∂–µ—Å—Ç—å {similarity:.3f} —Å –Ω–æ–≤–æ—Å—Ç—å—é {candidate['id']}")
                    return DuplicateResult(
                        is_duplicate=True,
                        existing_news_id=candidate['id'],
                        similarity_score=similarity,
                        similarity_type="myers",
                        reason=f"–í—ã—Å–æ–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É –ú–∞–π–µ—Ä—Å–∞ ({similarity:.3f})"
                    )
            
            return DuplicateResult(is_duplicate=False)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ú–∞–π–µ—Ä—Å–∞: {e}")
            return DuplicateResult(is_duplicate=False)
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏.
        
        Args:
            s1: –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞
            s2: –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞
            
        Returns:
            int: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
        """
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    async def _cosine_similarity_comparison(self, text: str, candidates: List[Dict]) -> DuplicateResult:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RuBERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.
        
        Args:
            text: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            candidates: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            
        Returns:
            DuplicateResult: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            text_embedding = await self._get_embedding(text)
            if text_embedding is None:
                return DuplicateResult(is_duplicate=False)
            
            best_similarity = 0.0
            best_candidate = None
            
            for candidate in candidates:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                candidate_text = f"{candidate['title']} {candidate['content']}"
                candidate_processed = await self._preprocess_text(candidate_text)
                candidate_embedding = await self._get_embedding(candidate_processed)
                
                if candidate_embedding is None:
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = cosine_similarity(
                    [text_embedding], 
                    [candidate_embedding]
                )[0][0]
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_candidate = candidate
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥
            if best_similarity > self.config.cosine_threshold:
                logger.info(f"üéØ –ö–æ—Å–∏–Ω—É—Å: —Å—Ö–æ–∂–µ—Å—Ç—å {best_similarity:.3f} —Å –Ω–æ–≤–æ—Å—Ç—å—é {best_candidate['id']}")
                return DuplicateResult(
                    is_duplicate=True,
                    existing_news_id=best_candidate['id'],
                    similarity_score=best_similarity,
                    similarity_type="cosine",
                    reason=f"–í—ã—Å–æ–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å ({best_similarity:.3f})"
                )
            
            return DuplicateResult(is_duplicate=False)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
            return DuplicateResult(is_duplicate=False)
    
    async def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ RuBERT.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            
        Returns:
            Optional[np.ndarray]: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if self.config.cache_embeddings:
                cache_key = f"embedding_{hashlib.md5(text.encode()).hexdigest()}"
                cached_embedding = self.cache.get(cache_key)
                if cached_embedding:
                    logger.debug("üíæ –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞")
                    return np.array(cached_embedding)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
            if not self._model_loaded:
                await self._load_rubert_model()
            
            if not self._model_loaded:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RuBERT –º–æ–¥–µ–ª—å")
                return None
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self._tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            with torch.no_grad():
                outputs = self._model(**inputs)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] —Ç–æ–∫–µ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                embedding = outputs.last_hidden_state[:, 0, :].numpy()[0]
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            embedding = embedding / np.linalg.norm(embedding)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if self.config.cache_embeddings:
                self.cache.set(cache_key, embedding.tolist(), expire_seconds=self.config.cache_ttl_hours * 3600)
            
            logger.debug(f"ü§ñ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω: {len(embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π")
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None
    
    async def _load_rubert_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç RuBERT –º–æ–¥–µ–ª—å (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)."""
        try:
            logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º RuBERT –º–æ–¥–µ–ª—å: {self.config.rubert_model}")
            
            self._tokenizer = AutoTokenizer.from_pretrained(self.config.rubert_model)
            self._model = AutoModel.from_pretrained(self.config.rubert_model)
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
            self._model.eval()
            
            self._model_loaded = True
            logger.info("‚úÖ RuBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RuBERT –º–æ–¥–µ–ª–∏: {e}")
            self._model_loaded = False
    
    async def _cluster_similar_news(self, text: str, candidates: List[Dict]) -> DuplicateResult:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ DBSCAN.
        
        Args:
            text: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            candidates: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            
        Returns:
            DuplicateResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        """
        try:
            if len(candidates) < self.config.dbscan_min_samples:
                return DuplicateResult(is_duplicate=False)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            embeddings = []
            valid_candidates = []
            
            for candidate in candidates:
                candidate_text = f"{candidate['title']} {candidate['content']}"
                candidate_processed = await self._preprocess_text(candidate_text)
                embedding = await self._get_embedding(candidate_processed)
                
                if embedding is not None:
                    embeddings.append(embedding)
                    valid_candidates.append(candidate)
            
            if len(embeddings) < self.config.dbscan_min_samples:
                return DuplicateResult(is_duplicate=False)
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è DBSCAN
            clustering = DBSCAN(
                eps=self.config.dbscan_eps,
                min_samples=self.config.dbscan_min_samples
            ).fit(embeddings)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –Ω–∞—à–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            text_embedding = await self._get_embedding(text)
            if text_embedding is None:
                return DuplicateResult(is_duplicate=False)
            
            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä
            best_similarity = 0.0
            best_candidate = None
            best_cluster_id = None
            
            for i, candidate in enumerate(valid_candidates):
                if clustering.labels_[i] == -1:  # –®—É–º
                    continue
                
                similarity = cosine_similarity([text_embedding], [embeddings[i]])[0][0]
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_candidate = candidate
                    best_cluster_id = clustering.labels_[i]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥
            if best_similarity > self.config.cosine_threshold:
                logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä: —Å—Ö–æ–∂–µ—Å—Ç—å {best_similarity:.3f} —Å –Ω–æ–≤–æ—Å—Ç—å—é {best_candidate['id']}")
                return DuplicateResult(
                    is_duplicate=True,
                    existing_news_id=best_candidate['id'],
                    similarity_score=best_similarity,
                    similarity_type="cluster",
                    reason=f"–ü—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∫–ª–∞—Å—Ç–µ—Ä—É –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π ({best_similarity:.3f})",
                    cluster_id=best_cluster_id
                )
            
            return DuplicateResult(is_duplicate=False)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return DuplicateResult(is_duplicate=False)
    
    async def merge_duplicate_sources(
        self, 
        existing_news_id: int, 
        new_source_id: int, 
        new_url: Optional[str] = None
    ) -> bool:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞.
        
        Args:
            existing_news_id: ID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –Ω–æ–≤–æ—Å—Ç–∏
            new_source_id: ID –Ω–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            new_url: URL –Ω–æ–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        """
        try:
            from src.models.database import NewsSource
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è —Å–≤—è–∑—å (–ø–æ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫—É)
            with self.db.get_session() as session:
                from src.models.database import Source
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫—É
                existing_relation = session.query(NewsSource).filter(
                    NewsSource.news_id == existing_news_id,
                    NewsSource.source_id == new_source_id
                ).first()
                
                if existing_relation:
                    logger.info(f"‚ÑπÔ∏è –°–≤—è–∑—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: –Ω–æ–≤–æ—Å—Ç—å {existing_news_id} + –∏—Å—Ç–æ—á–Ω–∏–∫ {new_source_id}")
                    return True
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ –∂–µ telegram_id –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏
                new_source = session.query(Source).filter(Source.id == new_source_id).first()
                if new_source:
                    existing_same_telegram = session.query(NewsSource, Source).join(
                        Source, NewsSource.source_id == Source.id
                    ).filter(
                        NewsSource.news_id == existing_news_id,
                        Source.telegram_id == new_source.telegram_id
                    ).first()
                    
                    if existing_same_telegram:
                        logger.info(f"‚ÑπÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫ —Å telegram_id '{new_source.telegram_id}' —É–∂–µ —Å–≤—è–∑–∞–Ω —Å –Ω–æ–≤–æ—Å—Ç—å—é {existing_news_id}")
                        return True
                
                # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –Ω–æ–≤–æ—Å—Ç—å—é –∏ –Ω–æ–≤—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º
                news_source = NewsSource(
                    news_id=existing_news_id,
                    source_id=new_source_id,
                    source_url=new_url
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                session.add(news_source)
                session.commit()
            
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–∏–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –Ω–æ–≤–æ—Å—Ç—å {existing_news_id} + –∏—Å—Ç–æ—á–Ω–∏–∫ {new_source_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            return False

